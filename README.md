<div align="center">

<!-- Animated Header -->
<img src="https://capsule-render.vercel.app/api?type=waving&color=0:667eea,100:764ba2&height=120&section=header" width="100%"/>

<!-- Logo -->
<img src="https://raw.githubusercontent.com/karasungur/visagen/main/assets/logo.png" alt="Visagen Logo" width="200"/>

<h1>
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=700&size=28&pause=1000&color=667EEA&center=true&vCenter=true&width=435&lines=Visagen;Modern+Face+Swapping;Built+with+PyTorch+Lightning" alt="Typing SVG" />
</h1>

<p><strong>Next-Generation Face Swapping Framework</strong></p>
<p><em>Powered by ConvNeXt, CBAM Attention & PyTorch Lightning</em></p>

<!-- Language Selector -->
<p>
  <a href="README.md">ğŸ‡ºğŸ‡¸ English</a> |
  <a href="README_TR.md">ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e</a>
</p>

<!-- Badges Row 1 -->
<p>
  <a href="https://github.com/karasungur/visagen/actions"><img src="https://img.shields.io/github/actions/workflow/status/karasungur/visagen/test.yml?branch=main&style=for-the-badge&logo=github&logoColor=white&label=CI" alt="Build Status"/></a>
  <img src="https://img.shields.io/badge/python-3.10%2B-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch"/>
  <img src="https://img.shields.io/badge/Lightning-2.0%2B-792EE5?style=for-the-badge&logo=lightning&logoColor=white" alt="Lightning"/>
</p>

<!-- Badges Row 2 -->
<p>
  <a href="LICENSE"><img src="https://img.shields.io/badge/license-MIT-00C853?style=for-the-badge" alt="License: MIT"/></a>
  <a href="https://github.com/karasungur/visagen/stargazers"><img src="https://img.shields.io/github/stars/karasungur/visagen?style=for-the-badge&logo=github&color=FFD700" alt="Stars"/></a>
  <a href="https://github.com/karasungur/visagen/network/members"><img src="https://img.shields.io/github/forks/karasungur/visagen?style=for-the-badge&logo=github&color=1E90FF" alt="Forks"/></a>
  <a href="https://github.com/karasungur/visagen/issues"><img src="https://img.shields.io/github/issues/karasungur/visagen?style=for-the-badge&logo=github&color=FF6B6B" alt="Issues"/></a>
</p>

<!-- Quick Navigation -->
<p>
  <a href="#-features">âœ¨ Features</a> â€¢
  <a href="#-installation">ğŸ“¦ Installation</a> â€¢
  <a href="#-quick-start">ğŸš€ Quick Start</a> â€¢
  <a href="#%EF%B8%8F-cli-tools">ğŸ› ï¸ CLI Tools</a> â€¢
  <a href="#%EF%B8%8F-architecture">ğŸ—ï¸ Architecture</a> â€¢
  <a href="#-contributing">ğŸ¤ Contributing</a>
</p>

<br/>

</div>

---

## ğŸ“– Overview

**Visagen** is a next-generation face swapping framework built from the ground up with modern deep learning practices. Inspired by DeepFaceLab, Visagen reimagines the entire pipeline using **PyTorch Lightning**, offering cleaner code, better performance, and easier extensibility.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VISAGEN PIPELINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“¥ Extract  â†’  ğŸ‹ï¸ Train  â†’  ğŸ¬ Swap  â†’  âœ¨ Postprocess         â”‚
â”‚      â”‚            â”‚           â”‚             â”‚                   â”‚
â”‚      â–¼            â–¼           â–¼             â–¼                   â”‚
â”‚  InsightFace   DFLModule    CBAM      Color Transfer            â”‚
â”‚  SegFormer     Lightning   Attention    Blending                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ§  Modern Architecture
- **ConvNeXt V2** encoder with GRN layers
- **CBAM** attention (Channel & Spatial)
- **Swish** activation for smooth gradients
- Skip connections for detail preservation

</td>
<td width="50%">

### ğŸ¯ Advanced Training
- Multi-loss: DSSIM, L1, LPIPS, ID, GAN
- Mixed precision (FP16/BF16)
- Gradient clipping & LR scheduling
- Eyes/Mouth & Gaze consistency loss

</td>
</tr>
<tr>
<td width="50%">

### ğŸ¨ Post-Processing
- 6 color transfer algorithms (RCT, LCT, SOT...)
- Neural color transfer (VGG-based)
- Laplacian, Poisson, Feather blending
- GFPGAN & GPEN face restoration

</td>
<td width="50%">

### âš¡ Production Ready
- ONNX & TensorRT export
- NVENC hardware encoding
- 30+ FPS inference
- 12 CLI tools

</td>
</tr>
</table>

---

## ğŸ“¦ Installation

### Requirements
- Python 3.10+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU acceleration)

### Basic Installation

```bash
# Clone the repository
git clone https://github.com/karasungur/visagen.git
cd visagen

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# Install base package
pip install -e .
```

### Full Installation (All Features)

```bash
pip install -e ".[full]"
```

<details>
<summary><b>ğŸ“‹ Optional Dependencies</b></summary>

```bash
# Vision (InsightFace, SegFormer)
pip install -e ".[vision]"

# Training (LPIPS)
pip install -e ".[training]"

# Hyperparameter Tuning (Optuna)
pip install -e ".[tuning]"

# Web Interface (Gradio)
pip install -e ".[gui]"

# Postprocessing (Color Transfer, Blending)
pip install -e ".[postprocess]"

# Video Merger (FFmpeg)
pip install -e ".[merger]"

# Model Export (ONNX, TensorRT)
pip install -e ".[export]"

# Face Restoration (GFPGAN)
pip install -e ".[restore]"

# GPU Data Loading (NVIDIA DALI)
pip install -e ".[dali]"
```

</details>

---

## ğŸš€ Quick Start

<details open>
<summary><b>ğŸ“¥ Step 1: Extract Faces</b></summary>

```bash
# Extract from video
visagen-extract \
    --input video.mp4 \
    --output-dir ./workspace/data_src/aligned \
    --face-size 512

# Extract from images
visagen-extract \
    --input ./photos/ \
    --output-dir ./workspace/data_dst/aligned \
    --face-size 512
```

</details>

<details open>
<summary><b>ğŸ‹ï¸ Step 2: Train Model</b></summary>

```bash
visagen-train \
    --src-dir ./workspace/data_src/aligned \
    --dst-dir ./workspace/data_dst/aligned \
    --output-dir ./workspace/model \
    --epochs 500 \
    --batch-size 8 \
    --resolution 512
```

</details>

<details>
<summary><b>ğŸ”§ Step 3: Hyperparameter Tuning (Optional)</b></summary>

```bash
visagen-tune \
    --src-dir ./workspace/data_src/aligned \
    --dst-dir ./workspace/data_dst/aligned \
    --output-dir ./workspace/optuna \
    --n-trials 20 \
    --epochs-per-trial 50
```

</details>

<details>
<summary><b>ğŸ¬ Step 4: Merge Faces into Video</b></summary>

```bash
# Basic merge with trained model
visagen-merge input.mp4 output.mp4 -c ./workspace/model/model.ckpt

# With face restoration and hardware encoding
visagen-merge input.mp4 output.mp4 -c model.ckpt \
    --restore-face --restore-strength 0.7 \
    --codec h264_nvenc --color-transfer rct
```

</details>

<details>
<summary><b>ğŸ“¦ Step 5: Export Model for Production</b></summary>

```bash
# Export to ONNX
visagen-export model.ckpt -o model.onnx --validate

# Export to TensorRT (FP16)
visagen-export model.onnx -o model.engine --format tensorrt --precision fp16
```

</details>

<details>
<summary><b>ğŸŒ Step 6: Launch Web Interface</b></summary>

```bash
visagen-gui --port 7860
```

</details>

---

## ğŸ—ï¸ Architecture

### Model Architecture

```
Input (512x512x3)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ConvNeXt   â”‚  â† Encoder (pretrained)
â”‚   Encoder    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    CBAM      â”‚  â† Channel & Spatial Attention
â”‚  Attention   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Swish     â”‚  â† Decoder with skip connections
â”‚   Decoder    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
Output (512x512x3)
```

---

## ğŸ› ï¸ CLI Tools

| Command | Description |
|:--------|:------------|
| ğŸ“¥ `visagen-extract` | Extract and align faces from images/videos |
| ğŸ‹ï¸ `visagen-train` | Train face swap model |
| ğŸ¯ `visagen-pretrain` | Pretrain encoder on FFHQ/CelebA |
| ğŸ”§ `visagen-tune` | Hyperparameter optimization (Optuna) |
| ğŸ¬ `visagen-merge` | Merge face swaps with NVENC encoding |
| ğŸ“¦ `visagen-export` | Export to ONNX/TensorRT |
| ğŸ“Š `visagen-sort` | Sort datasets (14 methods) |
| ğŸŒ `visagen-gui` | Launch Gradio web interface |
| ğŸï¸ `visagen-video` | Video frame extraction/creation |
| âœ¨ `visagen-enhance` | Batch face enhancement (GFPGAN/GPEN) |
| ğŸ“ `visagen-resize` | Resize faceset with metadata |
| âš¡ `visagen-benchmark` | Performance benchmarks |

---

<details>
<summary><b>ğŸ“ Project Structure</b></summary>

```
visagen/
â”œâ”€â”€ ğŸ“‚ data/               # Data loading & augmentation
â”‚   â”œâ”€â”€ dataset.py         # FaceDataset
â”‚   â”œâ”€â”€ datamodule.py      # FaceDataModule
â”‚   â”œâ”€â”€ dali_pipeline.py   # NVIDIA DALI GPU pipeline
â”‚   â””â”€â”€ augmentations.py
â”œâ”€â”€ ğŸ“‚ models/             # Neural network architectures
â”‚   â”œâ”€â”€ encoder.py         # ConvNeXt encoder
â”‚   â”œâ”€â”€ decoder.py         # Swish decoder
â”‚   â”œâ”€â”€ attention.py       # CBAM attention
â”‚   â””â”€â”€ discriminator.py
â”œâ”€â”€ ğŸ“‚ training/           # Training logic
â”‚   â”œâ”€â”€ dfl_module.py      # PyTorch Lightning module
â”‚   â”œâ”€â”€ pretrain_module.py # Pretraining module
â”‚   â””â”€â”€ losses.py          # Loss functions
â”œâ”€â”€ ğŸ“‚ merger/             # Video processing pipeline
â”‚   â”œâ”€â”€ video_io.py        # FFmpeg video I/O with NVENC
â”‚   â”œâ”€â”€ frame_processor.py # Single-frame processing
â”‚   â”œâ”€â”€ batch_processor.py # Parallel processing
â”‚   â””â”€â”€ merger.py          # High-level orchestration
â”œâ”€â”€ ğŸ“‚ postprocess/        # Post-processing
â”‚   â”œâ”€â”€ color_transfer.py  # RCT, LCT, SOT, MKL, IDT algorithms
â”‚   â”œâ”€â”€ neural_color.py    # VGG-based neural color transfer
â”‚   â”œâ”€â”€ blending.py        # Laplacian, Poisson, Feather
â”‚   â”œâ”€â”€ restore.py         # GFPGAN face restoration
â”‚   â””â”€â”€ gpen.py            # GPEN face restoration
â”œâ”€â”€ ğŸ“‚ export/             # Model export
â”‚   â”œâ”€â”€ onnx_exporter.py   # ONNX export
â”‚   â”œâ”€â”€ tensorrt_builder.py# TensorRT engine builder
â”‚   â””â”€â”€ validation.py      # Export validation
â”œâ”€â”€ ğŸ“‚ sorting/            # Dataset sorting
â”‚   â””â”€â”€ sorter.py          # 14 sorting methods
â”œâ”€â”€ ğŸ“‚ tuning/             # Hyperparameter optimization
â”‚   â””â”€â”€ optuna_tuner.py
â”œâ”€â”€ ğŸ“‚ tools/              # CLI tools
â”‚   â”œâ”€â”€ extract_v2.py      # Face extraction
â”‚   â”œâ”€â”€ train.py           # Training
â”‚   â”œâ”€â”€ pretrain.py        # Pretraining
â”‚   â”œâ”€â”€ merge.py           # Video merging
â”‚   â”œâ”€â”€ export.py          # Model export
â”‚   â”œâ”€â”€ sorter.py          # Dataset sorting
â”‚   â”œâ”€â”€ tune.py            # HPO
â”‚   â”œâ”€â”€ video_ed.py        # Video frame tools
â”‚   â”œâ”€â”€ faceset_enhancer.py# Batch face enhancement
â”‚   â”œâ”€â”€ faceset_resizer.py # Faceset resizing
â”‚   â”œâ”€â”€ benchmark.py       # Performance benchmarks
â”‚   â””â”€â”€ gradio_app.py      # Web UI (10 tabs)
â”œâ”€â”€ ğŸ“‚ vision/             # Computer vision
â”‚   â”œâ”€â”€ detector.py        # InsightFace SCRFD detection
â”‚   â”œâ”€â”€ aligner.py         # Face alignment (Umeyama)
â”‚   â”œâ”€â”€ segmenter.py       # SegFormer segmentation
â”‚   â”œâ”€â”€ dflimg.py          # DFL image metadata
â”‚   â””â”€â”€ mask_export.py     # LabelMe/COCO export
â””â”€â”€ ğŸ“‚ tests/              # Unit tests (636+)
```

</details>

---

## ğŸ“Š Performance

<table>
<tr>
<td align="center">
<h3>ğŸš„ 50 img/s</h3>
<sub>Training Speed (RTX 3090)</sub>
</td>
<td align="center">
<h3>ğŸ’¾ 8 GB</h3>
<sub>VRAM (512Ã—512, batch=8)</sub>
</td>
<td align="center">
<h3>âš¡ 30 FPS</h3>
<sub>Inference Speed</sub>
</td>
<td align="center">
<h3>âœ… 636+</h3>
<sub>Unit Tests</sub>
</td>
</tr>
</table>

---

## ğŸ‘¥ Contributors

<a href="https://github.com/karasungur/visagen/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=karasungur/visagen" />
</a>

### Core Team

<table>
<tr>
<td align="center">
  <a href="https://github.com/karasungur">
    <img src="https://github.com/karasungur.png" width="100px;" alt="Mustafa Karasungur"/><br />
    <sub><b>Mustafa Karasungur</b></sub>
  </a><br />
  <sub>ğŸ—ï¸ Project Lead & Core Developer</sub>
</td>
</tr>
</table>

<p align="center">
  <i>Contributions are welcome! See the section below.</i>
</p>

---

## ğŸ¤ Contributing

We love contributions! Whether you're fixing bugs, improving documentation, or proposing new features, your help is welcome.

<details>
<summary><b>ğŸ“‹ Quick Start for Contributors</b></summary>

```bash
# Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/visagen.git
cd visagen

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac

# Development installation
pip install -e ".[dev]"

# Create a new branch
git checkout -b feature/your-feature-name
```

</details>

<details>
<summary><b>ğŸ§ª Running Tests</b></summary>

```bash
# Run all tests
pytest visagen/tests/ -v

# Run with coverage
pytest visagen/tests/ --cov=visagen --cov-report=html

# Run specific test file
pytest visagen/tests/test_forward_pass.py -v
```

</details>

<details>
<summary><b>ğŸ¨ Code Style</b></summary>

We use **Ruff** for linting and formatting:

```bash
# Check code style
ruff check visagen/

# Auto-format code
ruff format visagen/

# Fix auto-fixable issues
ruff check visagen/ --fix
```

</details>

For detailed guidelines, see our [**Contributing Guide**](CONTRIBUTING.md).

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [DeepFaceLab](https://github.com/iperov/DeepFaceLab) - Original inspiration
- [PyTorch Lightning](https://lightning.ai/) - Training framework
- [InsightFace](https://github.com/deepinsight/insightface) - Face detection
- [Optuna](https://optuna.org/) - Hyperparameter optimization

---

<div align="center">

<!-- Animated Footer -->
<img src="https://capsule-render.vercel.app/api?type=waving&color=0:667eea,100:764ba2&height=100&section=footer" width="100%"/>

<br/>

**Made with â¤ï¸ by [Mustafa Karasungur](https://github.com/karasungur)**

<sub>If you find this project useful, please consider giving it a â­</sub>

<br/>

<a href="https://github.com/karasungur/visagen/stargazers">
  <img src="https://img.shields.io/github/stars/karasungur/visagen?style=social" alt="GitHub Stars"/>
</a>

</div>
